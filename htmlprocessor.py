import os
import re
import logging
from typing import Dict, Optional, Tuple, List, Any
from bs4 import BeautifulSoup
import html2text
from urllib.parse import unquote
import unicodedata

from config import Config
from linkchecker import LinkChecker
from confluencetaghandler import convert_custom_tags_to_html

# Define comprehensive multilingual month mapping
MONTH_PATTERNS = {
    # English
    "Jan": "01", "Feb": "02", "Mar": "03", "Apr": "04", "May": "05",
    "Jun": "06", "Jul": "07", "Aug": "08", "Sep": "09", "Oct": "10",
    "Nov": "11", "Dec": "12",

    # German
    "Mai": "05", "Mär": "03", "Mrz": "03", "Okt": "10", "Dez": "12",

    # French
    "Janv": "01", "Févr": "02", "Fév": "02", "Mars": "03", "Avr": "04",
    "Juin": "06", "Juil": "07", "Août": "08", "Sept": "09", "Déc": "12",

    # Spanish
    "Ene": "01", "Abr": "04", "Ago": "08", "Dic": "12",

    # Italian
    "Gen": "01", "Mag": "05", "Giu": "06", "Lug": "07", "Ago": "08",
    "Set": "09", "Ott": "10", "Dic": "12",

    # Dutch
    "Mei": "05", "Mrt": "03", "Okt": "10"
}
FOOTER_PATTERN = r'\nDocument generated by Confluence on [A-Za-z]+\. \d{1,2}, \d{4} \d{1,2}:\d{2}\n\n\[Atlassian\]\(<https://www\.atlassian\.com/>\)\n*$'
INVALID_CHARS = re.compile(r'[+/\\:*?&"<>|^\[\]]')

class HtmlProcessor:
    def __init__(self, config: Config, logger: logging.Logger):
        """Setup configuration"""
        self.config = config
        self.logger = logger
        self.page_tag_mapping = {}  # Maps tags to page IDs
        self.blog_post_tags: Dict[str, List[str]] = {}  # Storage for blog post tags

    def _convert_blog_html_to_md(self, blog_post: dict, output_dir: str, link_checker: LinkChecker) -> str:
        """
        Convert a blog post's HTML body to Markdown and save it to a file.

        Args:
        blog_post: The blog post object with body content
        output_dir: The directory to save the Markdown file

        Returns:
        The path to the created Markdown file
        """
        self.logger.info(f"Converting blog post {blog_post['id']} to Markdown")

        # Extract HTML content from the blog post
        html_content: str = blog_post["bodypage"]["body"]

        # Remove CDATA wrapper if present
        if html_content.startswith("<![CDATA[") and html_content.endswith("]]>"):
            html_content = html_content[9:-3]

        # Convert custom tags to HTML first
        self.logger.debug(f"html_content before: {html_content}")
        html_content = convert_custom_tags_to_html(html_content, self.logger)
        self.logger.debug(f"html_content after: {html_content}")

        # DEBUG
        temp_results = link_checker.attachment_processor.xml_processor.get_all_related_pages(blog_post["id"])
        self.logger.debug(f"Related pages for blog post {blog_post['id']}: {temp_results}")

        # Remove content-by-label sections (tags already extracted during mapping phase)
        html_content = self._remove_content_by_label_sections(html_content)

        # Convert HTML to Markdown
        try:
            self.logger.debug("Converting HTML to Markdown")
            markdown_content = self._convert_html_to_markdown(html_content)
        except Exception as e:
            self.logger.error(f"Failed to convert blog post {blog_post['id']}: {str(e)}")
            raise Exception(f"Blog post conversion failed: {e}")

        # Create filename from blog post title
        filename = f"{blog_post['title']}.md"
        output_path = os.path.join(output_dir, filename)

        # Process attachment links in the blog post
        page_id = blog_post['id']

        self.logger.debug(f"Processing images, local attachments, and external links for blog post ID: {page_id}")    
        markdown_content = link_checker.process_invalid_video_links(html_content, markdown_content)
        markdown_content = link_checker.process_images(html_content, markdown_content)
        markdown_content = link_checker.process_attachment_links(markdown_content)

        # Add YAML header
        if self.config.YAML_HEADER_BLOG:
            # Combine YAML header and Markdown content
            markdown_content = self._insert_yaml_header_md_blogpost(markdown_content, blog_post, link_checker)

        # Save the markdown file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        self.logger.info(f"Saved blog post to: {output_path}")
        return output_path

    def _convert_html_to_markdown(self, html_content: str):
        """Convert HTML to Markdown"""
        try:
            self.logger.debug("Starting HTML to Markdown conversion")

            # Validate input
            if not html_content or not html_content.strip():
                self.logger.warning("Empty HTML content provided to converter")
                return ""

            # Configure html2text
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = False
            h.ignore_tables = False
            h.body_width = 0
            h.protect_links = True
            h.unicode_snob = True
            h.mark_code = True

            # Enhanced table settings
            h.pad_tables = True
            h.single_line_break = False
            h.wrap_links = False
            h.wrap_list_items = False
            h.escape_all = False
            h.bypass_tables = False
            h.ignore_emphasis = False
            h.skip_internal_links = False
            h.decode_errors = 'ignore'
            h.default_image_alt = ''

            # convert and return
            return h.handle(html_content)

        except Exception as e:
            self.logger.debug(f"Failed to convert HTML to Markdown: {e}")
            raise Exception(f"HTML to Markdown conversion failed: {e}")

    def _preprocess_tables(self, html_content):
        """Preprocess tables while preserving ALL content including nested elements"""
        soup = BeautifulSoup(html_content, 'html.parser')

        # First, convert Confluence attachments to clean links
        self._convert_confluence_attachments_to_links(soup)

        for table in soup.find_all('table'):
            # Remove table attributes that might confuse parsers
            table.attrs = {}

            # Handle nested tables by converting them to inline content
            for nested_table in table.find_all('table'):
                # Convert nested table to structured text that preserves content
                nested_content = []
                for nested_row in nested_table.find_all('tr'):
                    row_cells = []
                    for cell in nested_row.find_all(['td', 'th']):
                        # Get all content including links, images, etc.
                        cell_html = ''.join(str(content) for content in cell.contents)
                        row_cells.append(cell_html.strip())
                    if row_cells:
                        nested_content.append(' | '.join(row_cells))

                # Replace nested table with preserved content
                if nested_content:
                    replacement_div = soup.new_tag('div')
                    replacement_div.string = ' [Table: ' + ' // '.join(nested_content) + '] '
                    nested_table.replace_with(replacement_div)

            # Fix the main issue: Convert problematic tags inside cells to inline content
            for cell in table.find_all(['td', 'th']):
                # Remove cell attributes but keep content
                cell.attrs = {}

                # Convert <br/> tags to spaces (they cause line breaks in markdown)
                for br in cell.find_all('br'):
                    br.replace_with(' ')

                # Convert <p> tags to inline content (they cause line breaks)
                for p in cell.find_all('p'):
                    # Extract all content from p tag and replace with inline version
                    p_content = ''.join(str(content) for content in p.contents)
                    p.replace_with(p_content + ' ')

                # Handle other block elements that might cause line breaks
                for block_elem in cell.find_all(['div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                    block_content = ''.join(str(content) for content in block_elem.contents)
                    block_elem.replace_with(block_content + ' ')

                # Clean up excessive whitespace but preserve links/images/other inline elements
                # Only normalize text nodes, not HTML elements
                for content in cell.contents[:]:  # Use slice to avoid modification during iteration
                    if hasattr(content, 'strip') and isinstance(content, str):
                        # This is a text node - clean it up
                        cleaned = ' '.join(content.split())
                        if cleaned != content:
                            content.replace_with(cleaned)

                # Handle truly empty cells
                if not cell.get_text(strip=True) and not cell.find_all():
                    cell.string = " "

            # Ensure proper table structure
            if not table.find('tbody'):
                tbody = soup.new_tag('tbody')
                thead = table.find('thead')
                for tr in table.find_all('tr', recursive=False):
                    if not thead or tr.parent != thead:
                        tr.extract()
                        tbody.append(tr)
                table.append(tbody)

            # Fix header structure if needed
            if not table.find('thead'):
                tbody = table.find('tbody')
                if tbody and tbody.find('tr'):
                    first_row = tbody.find('tr')
                    # Check if first row looks like a header
                    has_th = bool(first_row.find('th'))

                    if has_th:
                        thead = soup.new_tag('thead')
                        first_row.extract()
                        thead.append(first_row)
                        table.insert(0, thead)

            # Ensure all rows have the same number of columns
            rows = table.find_all('tr')
            if rows:
                max_cols = max(len(row.find_all(['td', 'th'])) for row in rows)

                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    current_cols = len(cells)

                    # Add missing cells
                    while current_cols < max_cols:
                        cell_type = 'th' if row.parent and row.parent.name == 'thead' else 'td'
                        empty_cell = soup.new_tag(cell_type)
                        empty_cell.string = " "
                        row.append(empty_cell)
                        current_cols += 1

            # Handle colspan/rowspan
            for cell in table.find_all(['td', 'th']):
                if cell.get('colspan') or cell.get('rowspan'):
                    if 'colspan' in cell.attrs:
                        del cell.attrs['colspan']
                    if 'rowspan' in cell.attrs:
                        del cell.attrs['rowspan']

        return str(soup)

    def _convert_confluence_attachments_to_links(self, soup: BeautifulSoup) -> None:
        """Convert Confluence attachment elements to direct markdown links"""

        # 1. Handle file attachments (PDFs, documents, etc.)
        for attachment in soup.find_all('a', class_='confluence-embedded-file'):
            # Extract the necessary information
            href = attachment.get('href') or attachment.get('data-file-src', '')

            # Get the actual link text from the attachment element
            # Look for text content, but avoid using aria-label
            link_text = ""

            # Try to extract meaningful text from the attachment
            img_element = attachment.find('img')
            if img_element and img_element.get('alt'):
                link_text = img_element.get('alt')
            elif attachment.get_text(strip=True):
                link_text = attachment.get_text(strip=True)
            else:
                # Last resort: use aria-label
                link_text = attachment.get('aria-label', '')

            # If still no text, use filename from href
            if not link_text and href:
                link_text = os.path.basename(href.split('?')[0])

            # Clean the href (remove parameters after ?)
            if href and '?' in href:
                href = href.split('?')[0]

            # Process internal attachment links
            if href and not href.startswith(('http://', 'https://')):
                # Ensure the href starts with the pattern linkchecker expects
                if not href.startswith('/download/attachments'):
                    if href.startswith('download/attachments'):
                        href = '/' + href
                    elif 'download/attachments' in href:
                        match = re.search(r'(download/attachments/\d+/[^?]+)', href)
                        if match:
                            href = '/' + match.group(1)

            # Create direct markdown link
            if href and link_text:
                markdown_link = f"[{link_text}]({href})"

                # Replace the entire confluence-embedded-file-wrapper
                wrapper = attachment.find_parent('span', class_='confluence-embedded-file-wrapper')
                if wrapper:
                    wrapper.replace_with(soup.new_string(markdown_link))
                else:
                    attachment.replace_with(soup.new_string(markdown_link))

        # 2. Handle embedded images
        for img in soup.find_all('img', class_='confluence-embedded-image'):
            src = img.get('src') or img.get('data-image-src', '')

            # Get the image description/alt text
            alt_text = (img.get('data-linked-resource-default-alias') or
                    img.get('alt') or
                    '')

            # If no alt text, use filename from src
            if not alt_text and src:
                alt_text = os.path.basename(src.split('?')[0])

            # Clean the src (remove parameters after ?)
            if src and '?' in src:
                src = src.split('?')[0]

            # Process internal image links
            if src and not src.startswith(('http://', 'https://')):
                # Ensure the src starts with the correct pattern
                if not src.startswith('/'):
                    if src.startswith('attachments/'):
                        src = '/' + src
                    elif 'attachments/' in src:
                        match = re.search(r'(attachments/\d+/[^?]+)', src)
                        if match:
                            src = '/' + match.group(1)

            # Create direct markdown image link
            if src and alt_text:
                markdown_link = f"![{alt_text}]({src})"

                # Replace the entire confluence-embedded-file-wrapper
                wrapper = img.find_parent('span', class_='confluence-embedded-file-wrapper')
                if wrapper:
                    wrapper.replace_with(soup.new_string(markdown_link))
                else:
                    img.replace_with(soup.new_string(markdown_link))

        # 3. Handle video elements
        for video in soup.find_all('video'):
            src = video.get('src', '')

            # Try to get a meaningful name for the video
            video_name = ""

            # Check for data attributes that might contain the filename
            for attr in ['data-linked-resource-default-alias', 'data-title', 'title']:
                if video.get(attr):
                    video_name = video.get(attr)
                    break

            # If no name found, extract from src
            if not video_name and src:
                video_name = os.path.basename(src.split('?')[0])

            # Clean the src (remove parameters after ?)
            if src and '?' in src:
                src = src.split('?')[0]

            # Process internal video links
            if src and not src.startswith(('http://', 'https://')):
                # Handle download/ prefix
                if src.startswith('download/'):
                    src = '/' + src
                elif not src.startswith('/') and 'attachments/' in src:
                    match = re.search(r'(attachments/\d+/[^?]+)', src)
                    if match:
                        src = '/' + match.group(1)

            # Create direct markdown link for video
            if src and video_name:
                markdown_link = f"[{video_name}]({src})"

                # Replace the video element
                wrapper = video.find_parent('span', class_='confluence-embedded-file-wrapper')
                if wrapper:
                    wrapper.replace_with(soup.new_string(markdown_link))
                else:
                    video.replace_with(soup.new_string(markdown_link))

        # 4. Handle external links (keep them as-is but clean up)
        for link in soup.find_all('a'):
            href = link.get('href', '')

            # Skip if already processed or if it's an internal confluence link
            if (link.get('class') and 'confluence-embedded-file' in link.get('class', [])):
                continue

            # Only process external links (http/https)
            if href.startswith(('http://', 'https://')):
                link_text = link.get_text(strip=True)

                # Clean up the link but keep it as HTML for html2text to process normally
                if link_text and href:
                    # Remove any confluence-specific classes but keep the link structure
                    link.attrs = {'href': href}
                    # Ensure clean text content
                    link.clear()
                    link.string = link_text

        # 5. Handle any remaining confluence-specific elements
        # Remove confluence-specific wrapper spans that might be empty now
        for wrapper in soup.find_all('span', class_='confluence-embedded-file-wrapper'):
            if not wrapper.get_text(strip=True) and not wrapper.find_all():
                wrapper.decompose()

        # 6. Handle macro placeholders and other confluence elements
        for macro in soup.find_all(['ac:structured-macro', 'ac:parameter', 'ac:rich-text-body']):
            # Convert macro content to simple text or remove if empty
            macro_text = macro.get_text(strip=True)
            if macro_text:
                macro.replace_with(soup.new_string(macro_text))
            else:
                macro.decompose()

    def _remove_link_list_on_top(self, markdown_content: str) -> str:
        """
        Removes any content that appears before the first heading in markdown content.
        Finds the first line starting with '#' and returns all content from that point forward.

        Args:
            markdown_content (str): The original markdown content

        Returns:
            str: The cleaned markdown content starting with the first heading
        """
        # Find the first line that starts with '#'
        lines = markdown_content.split('\n')
        start_index = 0

        for i, line in enumerate(lines):
            if line.strip().startswith('#'):
                start_index = i
                break

        # Return all content starting from the first heading
        result = '\n'.join(lines[start_index:])

        return result

    def _remove_space_details(self, markdown_content: str) -> str:
        """
        Removes the "#  Space Details:" section from markdown content, stopping at the first H2 or next H1.
        Preserves all other sections, including "## Available Pages:" and other H1/H2 headers.

        Args:
            markdown_content (str): The original markdown content

        Returns:
            str: The cleaned markdown content with the Space Details section removed
        """
        self.logger.debug("Removing space details header")
        space_header = self.config.SPACE_DETAILS_SECTION

        # Check if the Space Details header exists
        if space_header not in markdown_content:
            # If not found, return the original content unchanged
            return markdown_content
        
        # Split the content at the Space Details header
        parts = markdown_content.split(space_header, 1)
        before_header = parts[0]
        after_header = parts[1]

        # Find the next H1 or H2 header
        lines = after_header.split('\n')
        end_index = len(lines)  # Default to end of section

        for i, line in enumerate(lines):
            # Check if this line starts a new H1 or H2 section
            if line.strip().startswith('#'):
                end_index = i
                break

        # Reconstruct the content without the Space Details section
        if end_index < len(lines):
            # There is another section after Space Details
            result = before_header + '\n'.join(lines[end_index:])
        else:
            # Space Details was the only section
            result = before_header.rstrip()

        return result

    def _remove_confluence_footer(self, markdown_content: str) -> str:
        """Remove the standard Confluence footer from markdown content"""
        self.logger.debug("Removing Confluence footer from markdown content")
        
        # Remove the footer
        cleaned_content = re.sub(FOOTER_PATTERN, '', markdown_content)
        return cleaned_content

    def _remove_markdown_section(self, markdown_content: str, section_header: str) -> str:
        """
        Remove a specific markdown section and all its content including subsections.
        Stops when it encounters another section at the same level or higher.

        Args:
            markdown_content: The markdown content to process
            section_header: The section header to remove (e.g., "## Attachments:")
                            Must include the heading markers (# or ##)

        Returns:
            The markdown content with the specified section removed

        Example:
            _remove_markdown_section(content, "## Attachments:")
            _remove_markdown_section(content, "## Space contributors")
            _remove_markdown_section(content, "# Any other Header")
        """
        self.logger.debug(f"Removing section '{section_header}' from markdown content")

        # Check if the section exists
        if section_header not in markdown_content:
            self.logger.debug(f"No '{section_header}' section found")
            return markdown_content

        # Determine the heading level (count the leading # symbols)
        heading_level = 0
        for char in section_header:
            if char == '#':
                heading_level += 1
            elif char != '#':
                # Stop if we hit any other character
                break

        # Split content at the section header
        parts = markdown_content.split(section_header, 1)
        before_section = parts[0]
        section_and_after = section_header + parts[1]

        # Find the next section at the same level or higher
        lines = section_and_after.split('\n')
        end_index = len(lines)  # Default to end of document

        for i, line in enumerate(lines[1:], 1):  # Skip the section header line
            # Check if this line starts a new section
            line_stripped = line.strip()
            if line_stripped.startswith('#'):
                # Count the # symbols until the first space
                line_heading_level = 0
                for char in line_stripped:
                    if char == '#':
                        line_heading_level += 1
                    elif char == ' ':
                        # Stop counting when we hit a space
                        break
                    else:
                        # Stop if we hit any other character
                        break

                # If this heading is at the same level or higher, stop here
                if line_heading_level <= heading_level:
                    end_index = i
                    break

        # Reconstruct the content without the removed section
        if end_index < len(lines):
            # There is a section after the removed one
            cleaned_content = before_section + '\n'.join(lines[end_index:])
        else:
            # The removed section was the last section
            cleaned_content = before_section.rstrip()

        # Check if we made changes
        if cleaned_content != markdown_content:
            self.logger.debug(f"'{section_header}' section removed")

        return cleaned_content

    def _remove_markdown_lines(self, markdown_content: str, lines_to_remove: list[str]) -> str:
        """
        Removes specific lines from the markdown content, handling potential surrounding whitespace
        and ensuring correct newline handling. It removes all occurrences, including consecutive ones.

        Args:
            markdown_content: The original markdown content as a string.
            lines_to_remove: A list of exact string lines to be removed (e.g., ["Merken"]).

        Returns:
            The markdown content with the specified lines removed.
        """
        if not lines_to_remove or not markdown_content:
            # If there's nothing to remove, return the original content
            return markdown_content

        # Escape potential regex special characters in the lines to remove
        # and strip whitespace from the config values for robust matching.
        # Filter out any empty strings resulting from stripping.
        patterns = [re.escape(line.strip()) for line in lines_to_remove if line.strip()]

        if not patterns:
            # If lines_to_remove only contained whitespace or was empty after stripping
            self.logger.debug("No valid non-whitespace patterns provided in lines_to_remove.")
            return markdown_content

        # Construct the regex pattern:
        # ^                  - Anchor to the start of a line (due to re.MULTILINE flag).
        # \s*                - Match optional leading whitespace on the line.
        # (?:pattern1|...)   - Non-capturing group for all escaped patterns, joined by OR (|).
        # \s*                - Match optional trailing whitespace on the line.
        # (?:\r\n|\r|\n)?    - Match an optional universal newline sequence (\r\n, \r, or \n).
        #                      The '?' makes it optional, correctly handling the last line of the file
        #                      whether it has a trailing newline or not.
        combined_pattern = r'^\s*(?:' + '|'.join(patterns) + r')\s*(?:\r\n|\r|\n)?'

        # Store original length for comparison later
        original_length = len(markdown_content)

        # Perform the substitution using re.sub.
        # The re.MULTILINE flag ensures '^' matches the start of each line.
        # We replace the entire matched pattern (line + optional newline) with an empty string.
        try:
            cleaned_content = re.sub(combined_pattern, '', markdown_content, flags=re.MULTILINE)
        except re.error as e:
            self.logger.error(f"Regex error during line removal: {e} with pattern: {combined_pattern}")
            # Return original content if regex fails to prevent data loss
            return markdown_content

        # Log if changes were made
        if len(cleaned_content) < original_length:
            # We can't easily count lines removed with regex without splitting again,
            # so just log that *some* removal occurred based on the criteria.
            self.logger.debug(f"Removed some lines matching criteria: {lines_to_remove}")
        else:
            self.logger.debug(f"No lines found matching criteria: {lines_to_remove}")

        return cleaned_content

    def _remove_embedded_icon_in_home_link(self, markdown_content: str) -> str:
        """
        Remove embedded icon in the home link in the h2 section.

        Before: "* [[Startpage.md|Startpage]] ![](images/icons/contenttypes/home_page_16.png)"
        After: "* [[Startpage.md|Startpage]]"

        Args:
            markdown_content: The markdown content to process

        Returns:
            The processed markdown content with embedded icons removed from home links
        """
        self.logger.debug("Removing embedded icons in home link")

        lines = markdown_content.splitlines()
        icon_link = ' ![](images/icons/contenttypes/home_page_16.png)'

        # Find the h2 section index
        h2_index = -1
        for i, line in enumerate(lines):
            if line.startswith('## '):
                h2_index = i
                self.logger.debug(f"Found h2 section at line {i}: {line}")
                break

        if h2_index == -1:
            self.logger.debug("No h2 section found")
            return markdown_content

        # Check the line after h2 (or the line after that if the next line is empty)
        target_index = h2_index + 1
        if target_index < len(lines) and not lines[target_index].strip():
            target_index += 1

        # Check if we have a valid line to process
        if target_index < len(lines):
            target_line = lines[target_index]
            self.logger.debug(f"Checking line {target_index}: {target_line}")

            # Check if this line contains the icon
            if icon_link in target_line:
                self.logger.debug(f"Found icon to remove in line: {target_line}")
                lines[target_index] = target_line.replace(icon_link, '')

        return '\n'.join(lines)

    def _replace_first_header_name(self, markdown_content: str, new_filename: str) -> str:
        """
        Replace the first h1 section name with the filename.

        Before: "#  Spacename : YourHeaderText "
        After: "# YourHeaderText"

        Args:
            markdown_content: The markdown content to process
            new_filename: The export path of the file

        Returns:
            The processed markdown content with the first h1 header replaced
        """
        self.logger.debug(f"Replacing header name with: {new_filename}")
        # Extract the filename from the path
        # Strip the path prefix (output\SOMETHING\) and the .md extension
        filename = os.path.basename(new_filename)
        if filename.endswith('.md'):
            filename = filename[:-3]  # Remove .md extension

        # Process line by line
        lines = markdown_content.splitlines()
        for i, line in enumerate(lines):
            if line.startswith('# '):
                # Replace with just the filename as header
                lines[i] = f"# {filename}"
                break  # Only process the first h1 header

        return '\n'.join(lines)

    def _remove_created_by(self, markdown_content: str, return_line: bool = True) -> tuple[str, str]:
        """
        Remove the 'Created by' line from markdown content and return both the cleaned content
        and the removed line.

        There is only one such line in the document, which can appear anywhere.

        Args:
            markdown_content: The original markdown content
            return_line: If True, return the removed line as the second element of the tuple

        Returns:
            A tuple containing (cleaned_content, removed_line)
            If no line was removed or return_line is False, removed_line will be an empty string

        Examples of lines to remove:
        - 'Created by any name goes here, last modified on Dec 29, 2021'
        - 'Created by Unbekannter Benutzer (abc123), last modified on Feb. 01, 2017'
        - 'Created by Unbekannter Benutzer (otherusername) on Apr 25, 2019'
        - 'Created by Unbekannter Benutzer (anyone), last modified by other user name on Jan. 30, 2025'
        """
        self.logger.debug("Removing 'Created by' line from markdown content")

        # Pattern to match lines starting with 'Created by' (with one or more spaces) and containing date information
        created_by_pattern = r'Created by\s+.*(?:on|last modified).*\d+.*'

        created_by_line = ""
        lines = markdown_content.splitlines()
        i = 0

        # Find the 'Created by' line
        while i < len(lines):
            if re.match(created_by_pattern, lines[i]):
                if return_line:
                    created_by_line = lines[i]

                # Remove the line
                lines.pop(i)

                # Also remove any blank line that follows the 'Created by' line
                if i < len(lines) and lines[i].strip() == "":
                    lines.pop(i)

                break  # Exit loop after finding the first match
            i += 1

        # Reconstruct the cleaned content
        cleaned_content = '\n'.join(lines)

        return cleaned_content, created_by_line

    def _insert_yaml_header_md(self, markdown_content: str, page_id: str, link_checker: LinkChecker) -> str:
        """
        Insert a YAML header at the beginning of the markdown content with information
        extracted from XML data if available, or from the 'Created by' line and file path.

        Args:
            markdown_content: The original markdown content
            page_id: The ID of the page
            link_checker: Used to get the right name for the up field

        Returns:
            The markdown content with the YAML header prepended
        """
        self.logger.debug(f"Inserting YAML header into markdown content for page ID: {page_id}")

        # Start with the template from config
        yaml_header = self.config.YAML_HEADER

        # Extract author from created_by_line
        default_author = "unknown"
        default_date_created = "1999-12-31"  # Default date
        author = default_author
        date_created = default_date_created
        parent_folder = self.config.DEFAULT_UP_FIELD

        # If we found a page ID, get its information
        if page_id:
            page_info = link_checker.attachment_processor.xml_processor.get_page_by_id(page_id)

            if page_info:
                # Get creator name
                if page_info.get("creatorId"):
                    author_id = page_info["creatorId"]
                    author_info = link_checker.attachment_processor.xml_processor.get_user_by_id(author_id)
                    author = author_info["name"]
                    self.logger.debug(f"Got author name: {author}")

                # Get creation date
                if page_info.get("creationDate"):
                    date_match = re.match(r'(\d{4})-(\d{2})-(\d{2})', page_info["creationDate"])
                    if date_match:
                        year, month, day = date_match.groups()
                        date_created = f"{year}-{month}-{day}"
                        self.logger.debug(f"Got creation date from XML: {date_created}")

                # Get parent title directly from the cached information
                parent_title = link_checker.attachment_processor.xml_processor.get_parent_title_by_id(page_id)
                if parent_title:
                    parent_folder = parent_title
                    self.logger.debug(f"Updated parent name to: {parent_folder}")
        else:
            self.logger.debug(f"Could not find page info: {parent_folder}")
        
        # Replace placeholders in the YAML header
        yaml_header = yaml_header.replace('author: [username]', f'author: {author}')
        yaml_header = yaml_header.replace('dateCreated: [date_created]', f'dateCreated: {date_created}')
        yaml_header = yaml_header.replace('[[up_field]]', f'[[{parent_folder}]]')
        
        # Get tags for this page
        if page_id:
            page_tags = self._get_page_tags(page_id)

        # Add tags if any exist
        if page_tags:
            # Replace the empty tags section with actual tags
            tags_section = 'tags:\n' + '\n'.join(f'  - "{tag}"' for tag in page_tags)
            yaml_header = yaml_header.replace('tags:\n  - ""', tags_section)
            self.logger.debug(f"Added {len(page_tags)} tags to YAML header")
        else:
            # Keep the empty tags section as is for consistency
            self.logger.debug("No tags found - keeping empty tags section")

        # Add the YAML header to the markdown content
        updated_content = yaml_header + '\n\n' + markdown_content

        return updated_content

    def _insert_yaml_header_md_index(self, markdown_content: str, page_id: str, link_checker: LinkChecker) -> str:
        """
        Insert a YAML header at the beginning of index markdown files with information
        extracted from the Space Details table and file path.

        Args:
            markdown_content: The original markdown content
            page_id: The ID of the page to which the YAML header will be added
            link_checker: Used to get the right name for the up field

        Returns:
            The markdown content with the YAML header added
        """
        self.logger.debug(f"Inserting YAML header into index markdown content for page ID: {page_id}")

        # Start with the template from config
        yaml_header = self.config.YAML_HEADER

        # Default values
        default_author = "unknown"
        default_date_created = "1999-12-31"  # Default date
        author = default_author
        date_created = default_date_created
        parent_folder = "" # should be empty, as it's the highest level (alt: self.config.DEFAULT_UP_FIELD)

        # Try to get information from XML if available
        if link_checker.attachment_processor.xml_processor is not None:
            self.logger.debug(f"Using XML Checker to get Header info")

            # If we found a page ID, get its information
            if page_id:
                space_id = link_checker.attachment_processor.xml_processor.get_space_id_by_page_id(page_id)
                self.logger.debug(f"Retrieved space ID: {space_id}")
                if space_id:
                    space_info = link_checker.attachment_processor.xml_processor.get_space_by_id(space_id)
                    self.logger.debug("Retrieved space info by space ID")
                if not space_info:
                    space_info = link_checker.attachment_processor.xml_processor.get_page_by_id(page_id)
                    self.logger.debug("Fallback to page info")
                if not space_info:
                    self.logger.debug(f"No space or page info found for page ID: {page_id}")
                if space_info:
                    # Get creator
                    if space_info.get("creatorId"):
                        author_id = space_info["creatorId"]
                        author_info = link_checker.attachment_processor.xml_processor.get_user_by_id(author_id)
                        author = author_info["name"]
                        self.logger.debug(f"Got author name: {author}")

                    # Get creation date
                    if space_info.get("creationDate"):
                        date_match = re.match(r'(\d{4})-(\d{2})-(\d{2})', space_info["creationDate"])
                        if date_match:
                            year, month, day = date_match.groups()
                            date_created = f"{year}-{month}-{day}"
                            self.logger.debug(f"Got creation date from XML: {date_created}")

                else:
                    self.logger.debug(f"Could not find page info for page ID: {page_id}")

        # Fall back to extracting from Space Details table if XML data wasn't available
        if author == default_author or date_created == default_date_created:
            extracted_author, extracted_date, _ = self._extract_space_metadata(markdown_content)

            if extracted_author and author == default_author:
                author = extracted_author
                self.logger.debug(f"Extracted author from Space Details: {author}")
            elif not extracted_author and author == default_author:
                self.logger.debug(f"Could not extract author from Space Details for ID: {page_id}. Using default: {default_author}")

            if extracted_date and date_created == default_date_created:
                date_created = extracted_date
                self.logger.debug(f"Extracted date from Space Details: {date_created}")
            elif not extracted_date and date_created == default_date_created:
                self.logger.debug(f"Could not extract date from Space Details for ID: {page_id}. Using default: {default_date_created}")

        # Replace placeholders in the YAML header
        yaml_header = yaml_header.replace('author: [username]', f'author: {author}')
        yaml_header = yaml_header.replace('dateCreated: [date_created]', f'dateCreated: {date_created}')
        yaml_header = yaml_header.replace('[[up_field]]', f'[[{parent_folder}]]')

        # Get tags for this page (index pages typically shouldn't have content-by-label tags)
        if page_id:
            page_tags = self._get_page_tags(page_id)
        
        # Add tags if any exist
        if page_tags:
            # Replace the empty tags section with actual tags
            tags_section = 'tags:\n' + '\n'.join(f'  - "{tag}"' for tag in page_tags)
            yaml_header = yaml_header.replace('tags:\n  - ""', tags_section)
            self.logger.debug(f"Added {len(page_tags)} tags to YAML header")
        else:
            # Keep the empty tags section as is for consistency
            self.logger.debug("No tags found - keeping empty tags section")
            
        # Add the YAML header to the markdown content
        updated_content = yaml_header + '\n\n' + markdown_content
        return updated_content

    def _insert_yaml_header_md_blogpost(self, markdown_content: str, blog_post: dict, link_checker: LinkChecker) -> str:
        """
        Insert a YAML header at the beginning of blog post markdown content with information
        extracted from XML data or other metadata.
        Args:
            markdown_content: The original markdown content
            blog_post: The blog post dictionary containing metadata
            link_checker: LinkChecker instance for XML access
        """
        # Get author name
        author = "unknown"
        if blog_post.get("creatorId"):
            author_info = link_checker.attachment_processor.xml_processor.get_user_by_id(blog_post["creatorId"])
            if author_info:
                author = author_info["name"]

        # Get creation date
        date_created = "1900-12-31"
        if blog_post.get("creationDate"):
            date_match = re.match(r'(\d{4})-(\d{2})-(\d{2})', blog_post["creationDate"])
            if date_match:
                year, month, day = date_match.groups()
                date_created = f"{year}-{month}-{day}"

        # Get space name as parent folder
        space_id = link_checker.attachment_processor.xml_processor.get_space_id_by_page_id(blog_post['id'])
        space_info = link_checker.attachment_processor.xml_processor.get_space_by_id(space_id)
        parent_id = space_info.get('homePageId', '')
        parent_folder = link_checker.attachment_processor.xml_processor.get_page_title_by_id(parent_id)
        self.logger.debug(f"Space ID: {space_id}, Parent ID: {parent_id}, Parent Folder: {parent_folder}")
        if parent_folder == None:
            parent_folder = ""
        self.logger.debug(f"Parent folder determined as: {parent_folder}")
        
        # Create YAML header
        yaml_header = self.config.YAML_HEADER_BLOG
        yaml_header = yaml_header.replace('author: [username]', f'author: {author}')
        yaml_header = yaml_header.replace('dateCreated: [date_created]', f'dateCreated: {date_created}')
        yaml_header = yaml_header.replace('[[up_field]]', f'[[{parent_folder}]]')

        # Get tags for this page
        if blog_post.get("id"):
            page_tags = self.get_blog_post_tags(blog_post['id'])
        
        # Add tags if any exist
        if page_tags:
            # Replace the empty tags section with actual tags
            tags_section = 'tags:\n' + '\n'.join(f'  - "{tag}"' for tag in page_tags)
            yaml_header = yaml_header.replace('tags:\n  - ""', tags_section)
            self.logger.debug(f"Added {len(page_tags)} tags to YAML header")
        else:
            # Keep the empty tags section as is for consistency
            self.logger.debug("No tags found - keeping empty tags section")
            
        # Combine YAML header and Markdown content
        markdown_content = yaml_header + "\n\n" + markdown_content

        # return results
        return markdown_content

    def _extract_space_metadata(self, markdown_content: str) -> tuple[str, str]:
        """
        Extract author, date information and space name from the Space Details table in markdown content.
        Returns a tuple of (author, date_created, space_name) or (None, None, None) if not found.
        """
        self.logger.debug("Extracting space metadata from markdown content")

        author = "unknown"
        date_created = None
        space_name = None
        space_header = self.config.SPACE_DETAILS_SECTION

        # Look for the Space Details header
        if space_header not in markdown_content:
            self.logger.debug("Space Details header not found")
            return None, None, None

        # Define regex patterns for different metadata fields
        name_pattern = re.compile(r'Name\s*\|\s*([^\n|]+)', re.MULTILINE)
        creator_pattern = re.compile(r'Created by\s*\|\s*([^\n|]+)', re.MULTILINE)

        # Extract space name
        name_match = name_pattern.search(markdown_content)
        if name_match:
            space_name = name_match.group(1).strip()
            self.logger.debug(f"Found space name: {space_name}")

        # Extract creator information
        creator_match = creator_pattern.search(markdown_content)
        if creator_match:
            creator_text = creator_match.group(1).strip()

            # Extract author name (before parentheses)
            if '(' in creator_text:
                author = creator_text.split('(')[0].strip()
            else:
                author = creator_text
            self.logger.debug(f"Found space creator: {author}")

            # Extract date
            date_match = re.search(r'\(([^)]+)\)', creator_text)
            if date_match:
                date_text = date_match.group(1).strip()

                # Handle various date formats
                # Format: "Feb. 03, 2017"
                month_abbr_pattern = re.compile(r'(\w+)\.?\s+(\d{1,2}),\s+(\d{4})')
                month_abbr_match = month_abbr_pattern.search(date_text)

                if month_abbr_match:
                    month_name = month_abbr_match.group(1)
                    day = month_abbr_match.group(2).zfill(2)  # Pad with leading zero if needed
                    year = month_abbr_match.group(3)
        
                if month_name in MONTH_PATTERNS:
                        month = MONTH_PATTERNS[month_name]
                        date_created = f"{year}-{month}-{day}"
                        self.logger.debug(f"Found space creation date: {date_created}")

        return author, date_created, space_name

    def _process_blog_posts(self, link_checker: LinkChecker) -> None:
        """
        Process all blog posts from XML and convert them to Markdown.
        """
        self.logger.info("Processing blog posts from XML...")

        # Collect all blog post IDs
        blog_post_ids = [
            page_id for page_id, page in link_checker.attachment_processor.xml_processor.page.items()
            if page.get("type") == "BlogPost"
        ]

        # Count total blog posts
        total_blog_posts = len(blog_post_ids)

        link_checker.attachment_processor.xml_processor.stats.total = total_blog_posts
        
        if total_blog_posts == 0:
            self.logger.info("No blog posts found to process")
            return
        else:
            self.logger.info(f"Found {total_blog_posts} blog posts to process")
        
        # Process each blog post
        for blog_id in blog_post_ids:
            blog_post = link_checker.attachment_processor.xml_processor.page[blog_id]
            space_id = blog_post.get("spaceId")

            # In case no spaceId is found
            if not space_id:
                self.logger.warning(f"Blog post {blog_id} has no space ID in page data")
                # Try to find space ID through other means
                space_id = link_checker.attachment_processor.xml_processor.find_space_id_for_blog(blog_id)
                if not space_id:
                    self.logger.warning(f"Could not find space ID for blog post {blog_id}")
                    link_checker.attachment_processor.xml_processor.stats.skip_file("Blog Posts")
                    continue
            
            space_key = link_checker.attachment_processor.xml_processor.get_space_by_id(space_id)
            if not space_key:
                self.logger.warning(f"Could not find space info for ID {space_id} (blog {blog_id})")
                link_checker.attachment_processor.xml_processor.stats.skip_file("Blog Posts")
                continue
            
            space_key = space_key.get("key", "unknown")
            if space_key == "unknown":
                self.logger.warning(f"Could not determine space key for space ID {space_id} (blog {blog_id})")
                link_checker.attachment_processor.xml_processor.stats.skip_file("Blog Posts")
                continue

            # Create the blog posts directory for this space
            blog_dir = os.path.join(self.config.OUTPUT_FOLDER, space_key, self.config.BLOGPOST_PATH)
            os.makedirs(blog_dir, exist_ok=True)

            # Skip if no body content
            if not blog_post.get("bodypage") or not blog_post["bodypage"].get("body"):
                self.logger.warning(f"Blog post with ID {blog_id} has no body content")
                link_checker.attachment_processor.xml_processor.stats.skip_file("Blog Posts")
                continue

            # Convert the blog post to Markdown
            try:
                md_path = self._convert_blog_html_to_md(blog_post, blog_dir, link_checker)
                link_checker.attachment_processor.xml_processor.stats.success += 1
                self.logger.debug(f"Successfully converted blog post {blog_id} to {md_path}")
            except Exception as e:
                self.logger.error(f"Failed to convert blog post {blog_id}: {str(e)}", exc_info=True)
                link_checker.attachment_processor.xml_processor.stats.failure += 1
                continue

            # Update progress
            link_checker.attachment_processor.xml_processor.stats.processed += 1
            link_checker.attachment_processor.xml_processor.stats.update_progress()

        # Update phase stats
        link_checker.attachment_processor.xml_processor.stats.update_phase_stats()
        self.logger.info(f"Blog post processing complete. Processed {link_checker.attachment_processor.xml_processor.stats.success} of {total_blog_posts} blog posts.")

    def _extract_tags_from_content_by_label_sections(self, html_content: str, link_checker: LinkChecker) -> None:
        """Extract tags from content-by-label sections, map them to target pages, and remove the sections."""
        soup = BeautifulSoup(html_content, 'html.parser')

        # Find content-by-label section
        content_by_label_sections = soup.find_all('ul', class_='content-by-label')
        
        for section in content_by_label_sections:
            items = section.find_all('li')
            
            for item in items:
                # Find the link to the target page
                link = item.find('a', href=True)
                if not link:
                    continue
                    
                target_href = link.get('href')
                
                # Find the label-details section for this item
                label_details = item.find('div', class_='label-details')
                if not label_details:
                    continue
                
                # Extract all tags from this item
                tag_links = label_details.find_all('a', rel='tag')
                tags = []
                for tag_link in tag_links:
                    tag_text = tag_link.get_text(strip=True)
                    if tag_text:
                        tags.append(tag_text)
                
                if tags and target_href:
                    # Convert href to page ID for reliable mapping
                    page_id = self._resolve_href_to_page_id(target_href, link_checker)
                    if page_id:
                        if page_id not in self.page_tag_mapping:
                            self.page_tag_mapping[page_id] = set()
                        self.page_tag_mapping[page_id].update(tags)
        
        #return str(soup)
    
    def _remove_content_by_label_sections(self, html_content: str) -> str:
        """Remove content-by-label sections from HTML content."""

        soup = BeautifulSoup(html_content, 'html.parser')

        # Remove all content-by-label sections
        content_by_label_sections = soup.find_all('ul', class_='content-by-label')
        for section in content_by_label_sections:
            section.decompose()
        
        # Return as string
        return str(soup)
    
    def _preprocess_blog_posts_in_current_page(self, html_content: str, page_path: str) -> str:      
        """
        Find blog posts in the current page, extract their tags, and replace with embedded links.
        
        Args:
            html_content: HTML content of the current page
            page_path: Path to the current HTML file being processed
            
        Returns:
            Modified HTML content with blog posts replaced by embedded links
        """
        soup = BeautifulSoup(html_content, 'html.parser')

        try:            
            # Find all blog-post-listing divs
            blog_listings = soup.find_all('div', class_='blog-post-listing')
            
            if not blog_listings:
                # No blog posts in this page, return original content as string
                return str(soup)
            
            self.logger.info(f"Found {len(blog_listings)} blog post(s) in {page_path}")
            
            for listing in blog_listings:
                # Extract blog post information
                blog_info = self._extract_blog_info_from_listing(listing, page_path)
                
                if blog_info:
                    blog_page_id = blog_info['page_id']
                    blog_title = blog_info['title']
                    tags = blog_info['tags']
                    
                    # Store tags for later use in _convert_blog_html_to_md
                    self.blog_post_tags[blog_page_id] = tags
                    
                    self.logger.debug(f"Extracted {len(tags)} tags for blog post '{blog_title}' (ID: {blog_page_id})")
                    
                    # Replace blog listing with embedded link
                    space_key = blog_info.get('space_key', '')
                    embedded_link_html = self._create_embedded_link_html(blog_title, space_key)
                    new_div = soup.new_tag('div', **{'class': 'embedded-blog-link'})
                    new_div.append(BeautifulSoup(embedded_link_html, 'html.parser'))
                    listing.replace_with(new_div)
            
            # Return as string
            return str(soup)
            
        except Exception as e:
            self.logger.error(f"Error preprocessing blog posts in page {page_path}: {e}")
            # Return as string
            return str(soup)
    
    def _extract_blog_info_from_listing(self, listing_div: BeautifulSoup, page_path: str) -> Optional[Dict[str, Any]]:
        """
        Extract blog post information from a blog-post-listing div.
        
        Args:
            listing_div: BeautifulSoup div element with class 'blog-post-listing'
            page_path: Path to the parent page
            
        Returns:
            Dictionary with blog info or None if extraction fails
        """
        try:
            # Extract the space key from the page path
            parts = page_path.split(os.sep)
            # Check if path starts with input folder name
            if len(parts) >= 2 and parts[0] == self.config.INPUT_FOLDER:
                space_key = parts[1]
            else:
                space_key = parts[0]

            # Set up the object
            blog_info = {
                'page_id': None,
                'title': None,
                'tags': [],
                'parent_page': page_path,
                'space_key': space_key
            }
            
            # Extract page ID and title from blog heading link
            blog_heading = listing_div.find('a', class_='blogHeading')
            if not blog_heading:
                self.logger.warning(f"No blog heading found in listing from {page_path}")
                return None
                
            href = blog_heading.get('href', '')
            title = blog_heading.get_text(strip=True)
            title = self._sanitize_filename(title)
            
            # Extract page ID from href (format: /pages/viewpage.action?pageId=32244149)
            page_id_match = re.search(r'pageId=(\d+)', href)
            if not page_id_match:
                self.logger.warning(f"Could not extract page ID from href: {href}")
                return None
                
            blog_info['page_id'] = page_id_match.group(1)
            blog_info['title'] = title
            
            # Extract tags from label-list in endsection
            endsection = listing_div.find('div', class_='endsection')
            if endsection:
                label_list = endsection.find('ul', class_='label-list')
                if label_list:
                    label_items = label_list.find_all('li', class_='aui-label')
                    for item in label_items:
                        link = item.find('a', class_='aui-label-split-main')
                        if link:
                            tag_text = link.get_text(strip=True)
                            if tag_text:
                                blog_info['tags'].append(tag_text)
            
            return blog_info
            
        except Exception as e:
            self.logger.error(f"Error extracting blog info from listing: {e}")
            return None

    def _create_embedded_link_html(self, blog_title: str, space_key: str) -> str:
        """
        Create HTML representation of embedded link for blog post.
        
        Args:
            blog_title: Title of the blog post
            blog_page_title: Title of the blog post
            
        Returns:
            HTML string for embedded link that will convert nicely to markdown
        """

        embedded_html = f"""
        <blockquote>
            <p>[!info]- {blog_title}<br>
            collapse: true<br>
            ![[{space_key}/blogposts/{blog_title}.md|{blog_title}]]</p>
        </blockquote>
        """
        
        return embedded_html
        
    def _resolve_href_to_page_id(self, href: str, link_checker: LinkChecker) -> str:
        """Resolve an href to a page ID using existing link processing logic."""
        # Handle /display/ links
        if '/display/' in href and not '/display/~' in href:
            parts = href.split('/display/', 1)[1].split('/', 1)
            if len(parts) == 2:
                space_key, page_title = parts
                page_title = page_title.replace('+', ' ')
                page_title = link_checker.attachment_processor.xml_processor._sanitize_filename(page_title)
                
                # Find page by space and title
                space_info = link_checker.attachment_processor.xml_processor.get_space_by_key(space_key)
                if space_info:
                    space_id = space_info.get('id')
                    for page_id, page_info in link_checker.attachment_processor.xml_processor.page.items():
                        if page_info.get('spaceId') == space_id and page_info.get('title') == page_title:
                            return page_id
        
        # Handle /pages/viewpage.action?pageId=X links
        if '/pages/viewpage.action' in href and 'pageId=' in href:
            import re
            page_id_match = re.search(r'pageId=(\d+)', href)
            if page_id_match:
                return page_id_match.group(1)
        
        return None

    def _get_page_tags(self, page_id: str) -> list:
        """Get tags for a specific page from the tag mapping."""
        if hasattr(self, 'page_tag_mapping') and page_id in self.page_tag_mapping:
            return sorted(list(self.page_tag_mapping[page_id]))
        return []
    
    def get_blog_post_tags(self, blog_page_id: str) -> List[str]:
        """
        Get tags for a blog post by its page ID.
        
        Args:
            blog_page_id: ID of the blog post
            
        Returns:
            List of tags for the blog post
        """
        return self.blog_post_tags.get(blog_page_id, [])
        
    def _is_special_folder(self, path: str) -> bool:
        """Check if a path contains any special folder names"""
        special_folders = [self.config.ATTACHMENTS_PATH, self.config.IMAGES_PATH, self.config.STYLES_PATH]
        return any(folder in path.split(os.sep) for folder in special_folders)

    def _get_special_folder_type(self, path: str) -> str:
        """Determine which type of special folder this is"""
        path_parts = path.split(os.sep)
        if self.config.STYLES_PATH in path_parts:
            return "styles"
        elif self.config.ATTACHMENTS_PATH in path_parts:
            return "attachments"
        elif self.config.IMAGES_PATH in path_parts:
            return "images"
        return None

    def _append_comments_to_page(self, markdown_content: str, page_info: dict) -> str:
        """Append comments to the page's markdown content, converting HTML to Markdown"""
        if not page_info or not page_info.get("comments"):
            return markdown_content

        # Only proceed if there are actually comments
        comments = page_info.get("comments", [])
        if not comments:
            return markdown_content
        
        # Process comments and collect valid ones
        valid_comments = []
        seen_comments = set()

        for comment in comments:
            # Get comment HTML content
            bodypage = comment.get("bodypage")
            if bodypage and bodypage.get("body"):
                comment_html = bodypage["body"]
                
                # Skip empty or whitespace-only content
                if not comment_html or not comment_html.strip():
                    self.logger.debug("Skipping empty comment HTML content")
                    continue
                
                # Create a hash of the comment HTML for deduplication
                comment_hash = hash(comment_html.strip())
                if comment_hash in seen_comments:
                    self.logger.debug("Skipping duplicate comment")
                    continue
                
                # Convert to Markdown
                try:
                    comment_md = self._convert_html_to_markdown(comment_html)
                    
                    # Only add if conversion produced meaningful content
                    if comment_md and comment_md.strip():
                        # Also check for duplicate markdown content
                        comment_md_stripped = comment_md.strip()
                        comment_md_hash = hash(comment_md_stripped)
                        
                        if comment_md_hash not in seen_comments:
                            valid_comments.append(comment_md_stripped)
                            seen_comments.add(comment_hash)
                            seen_comments.add(comment_md_hash)
                        else:
                            self.logger.debug("Skipping duplicate comment after markdown conversion")
                    else:
                        self.logger.debug("Comment conversion produced empty result, skipping")
                except IndexError as e:
                    self.logger.debug(f"Index error converting comment HTML: {str(e)}")
                    continue
                except Exception as e:
                    self.logger.debug(f"Failed to convert comment HTML: {str(e)}")
                    continue
        
        # Only add comments section if we have valid comments
        if valid_comments:
            comments_section = "\n\n## Comments\n\n"
            comments_section += "\n\n---\n\n".join(valid_comments)
            
            self.logger.debug(f"Added {len(valid_comments)} valid comments to page")
            return markdown_content + comments_section
        else:
            self.logger.debug("No valid comments to add")
            return markdown_content
    
    def _sanitize_filename(self, filename: str) -> str:
        """
        Consistently sanitize filenames for Obsidian compatibility.

        Combines regex efficiency with specific character handling for optimal
        performance and accuracy.
        """
        if not filename:
            self.logger.debug(f"Could not find a filename to sanitize: '{filename}'")
            return "unnamed"

        # Store input for logging
        original_filename = filename

        # URL decode the filename
        filename = unquote(filename)

        # Normalize Unicode characters
        filename = unicodedata.normalize('NFKC', filename)

        # Filter bad/invisible characters, apply URL decoding
        filename = ''.join(c for c in filename if self.is_valid_char(c))
        
        # Trim leading/trailing periods and spaces
        filename = filename.strip('. ')

        # Replace remaining problematic characters with dashes
        filename = re.sub(INVALID_CHARS, '-', filename)

        # Handle spaces according to configuration
        if self.config.USE_UNDERSCORE_IN_FILENAMES:
            filename = filename.replace(' ', '_')

        # Ensure the filename is not empty
        if not filename:
            self.logger.warning(f"Could not sanitize filename: '{original_filename}'")
            return original_filename

        self.logger.debug(f"Sanitized filename from '{original_filename}' to '{filename}'")
        return filename
        
    def is_valid_char(self, char):
        """
        Comprehensive character validation that combines all checks:
        - Unicode category validation
        - Private use area detection
        - Specific character exclusions
        """
        if not char:
            return False

        # Mapping of URL-encoded characters to their regular equivalents
        url_encoded_mapping = {
            '%20': ' ',    # Space
            '%3C': '<',    # Less than
            '%3E': '>',    # Greater than
            '%3A': ':',    # Colon
            '%22': '"',    # Double quote
            '%2F': '/',    # Forward slash
            '%5C': '\\',   # Backslash
            '%7C': '|',    # Vertical bar or pipe
            '%3F': '?',    # Question mark
            '%2A': '*'     # Asterisk
        }

        # Check for specific characters to exclude
        excluded_chars = {
            '\u200b',  # Zero width space
            '\u200c',  # Zero width non-joiner
            '\u200d',  # Zero width joiner
            '\u200e',  # Left-to-right mark
            '\u200f',  # Right-to-left mark
            '\ufeff'   # Byte order mark
        }
        
        # Replace URL-encoded characters with their regular equivalents
        if char in url_encoded_mapping:
            char = url_encoded_mapping[char]
        
        # Check for specific characters to exclude
        if char in excluded_chars:
            return False

        # Check for private use areas
        code_point = ord(char)
        if (0xE000 <= code_point <= 0xF8FF or          # Basic Multilingual Plane private use area
            0xF0000 <= code_point <= 0xFFFFD or        # Supplementary Private Use Area-A
            0x100000 <= code_point <= 0x10FFFD):       # Supplementary Private Use Area-B
            return False

        # Get Unicode category
        category = unicodedata.category(char)

        # Accepts these categories:
        # Cc: Other, Control - Non-printable control characters (e.g., \n, \r, \t)
        # Cf: Other, Format - Non-printable format characters (e.g., zero-width joiner, zero-width non-joiner)
        # Co: Other, Private Use - Characters reserved for private use, without standardized meaning
        # Cs: Other, Surrogate - Surrogate code points used in UTF-16 encoding, not valid on their own

        # Reject control characters
        if category in {'Cc', 'Cf', 'Co', 'Cs'}:
            return False

        # Accepts these categories:
        # Lu: Uppercase Letter
        # Ll: Lowercase Letter
        # Lt: Titlecase Letter
        # Lm: Modifier Letter
        # Lo: Other Letter
        # Nd: Decimal Number
        # Nl: Letter Number
        # No: Other Number
        # Pd: Dash Punctuation
        # Pe: Close Punctuation
        # Ps: Open Punctuation
        # Pi: Initial Punctuation
        # Pf: Final Punctuation
        # Pc: Connector Punctuation
        # Po: Other Punctuation
        # Sm: Math Symbol
        # Sc: Currency Symbol
        # Sk: Modifier Symbol
        # So: Other Symbol
        # Zs: Space Separator
        return category.startswith(('L', 'N', 'P', 'S', 'Z'))
    
    # Public
    def convert_html_to_md(self, html_file: str, md_output_name: str, link_checker: LinkChecker) -> bool:
        """
        Convert HTML to Markdown with intelligent filename handling.

        Args:
            html_file: Path to the HTML file to convert
            md_output_name: Target path for the Markdown output
            link_checker: LinkChecker instance for managing filename mappings

        Returns:
            bool: True if conversion was successful, False otherwise
        """
        try:
            self.logger.info(f"Starting conversion of {html_file}")

            # Extract page ID and name from filename
            filename = os.path.basename(html_file)

            # Skipping original index html file
            if filename == "index.html":
                self.logger.debug("Skipping original 'index.html' file. Index will be replaced by actual Homepage.")
                return True

            # Read HTML content
            with open(html_file, 'r', encoding='utf-8') as f:
                html_content = f.read()
            self.logger.debug(f"HTML file size: {len(html_content)} bytes")

            # Remove content-by-label sections (tags already extracted during mapping phase)
            html_content = self._remove_content_by_label_sections(html_content)

            # Preprocess blog posts in this page (extract tags and replace with embedded links)
            html_content = self._preprocess_blog_posts_in_current_page(html_content, html_file)

            # Convert HTML to Markdown
            try:
                self.logger.debug("Converting HTML to Markdown")
                    
                # Apply content-preserving preprocessing to fix tables
                html_content = self._preprocess_tables(html_content)

                # Convert to MD
                markdown_content = self._convert_html_to_markdown(html_content)

                # Remove existing comment section (because it's a table)
                section = '## Comments:'
                markdown_content = self._remove_markdown_section(markdown_content, section)

            except Exception as e:
                self.logger.error(f"Failed to convert {filename}: {str(e)}")
                return False

            # Extract page ID using the XML processor
            page_id = link_checker.attachment_processor.xml_processor.get_page_id_by_filename(filename, md_output_name)

            if page_id:
                page_info = link_checker.attachment_processor.xml_processor.get_page_by_id(page_id)

            # Append comment section
            if page_info:
                markdown_content = self._append_comments_to_page(markdown_content, page_info)

            # Check if it's the new index file
            space_key = os.path.basename(os.path.dirname(md_output_name))
            space_info = link_checker.attachment_processor.xml_processor.get_space_by_key(space_key)
            homePageId = space_info["homePageId"]
            is_new_index = homePageId == page_id
            if is_new_index:
                self.logger.debug("New index file detected.")

            # Get filename using XML data
            self.logger.debug(f"Attempting to get clean name for page '{filename}' from ID: '{page_id}'")

            page_title = link_checker.attachment_processor.xml_processor.get_page_title_by_id(page_id)
            self.logger.debug(f"Found new page title: '{page_title}'")
            final_md_output_name = f"{page_title}.md"

            # Remove header link list (except for index files)
            if is_new_index:
                self.logger.debug(f"Removing embedded icon in home link for: '{final_md_output_name}'")
                markdown_content = self._remove_embedded_icon_in_home_link(markdown_content)

            # For all files
            self.logger.debug(f"Removing link list for: '{final_md_output_name}'")
            markdown_content = self._remove_link_list_on_top(markdown_content)
            
            if page_title is not None:
                self.logger.debug(f"Matching h1 header text with new filename: '{page_title}'")
                markdown_content = self._replace_first_header_name(markdown_content, page_title)
            else:
                self.logger.debug(f"Filename not found in cache - skipping ID: '{page_id}'")

            # Process video links
            self.logger.debug("Processing video links")
            markdown_content = link_checker.process_invalid_video_links(html_content, markdown_content)

            # Process images and external links
            self.logger.debug(f"Processing images, local attachments, and external links for page ID: '{page_id}'")
            markdown_content = link_checker.process_images(html_content, markdown_content)
            markdown_content = link_checker.process_attachment_links(markdown_content)

            # Remove Confluence footer
            markdown_content = self._remove_confluence_footer(markdown_content)

            # Remove 'Created by' lines
            markdown_content, _ = self._remove_created_by(markdown_content, return_line=True)

            # Add YAML header
            if self.config.YAML_HEADER:
                if is_new_index:
                    self.logger.debug(f"Inserting YAML Header for index: '{final_md_output_name}'")
                    markdown_content = self._insert_yaml_header_md_index(markdown_content, page_id, link_checker)
                else:
                    self.logger.debug(f"Inserting YAML Header for file: '{final_md_output_name}'")
                    markdown_content = self._insert_yaml_header_md(markdown_content, page_id, link_checker)

            # Remove space details for index files
            if is_new_index:
                    self.logger.debug(f"Removing space details for index: '{final_md_output_name}'")
                    markdown_content = self._remove_space_details(markdown_content)

            # Remove unwanted sections
            if self.config.SECTIONS_TO_REMOVE:
                self.logger.debug("Removing unwanted sections")
                for section in self.config.SECTIONS_TO_REMOVE:
                    markdown_content = self._remove_markdown_section(markdown_content, section)

            # Remove unwanted lines
            if self.config.LINES_TO_REMOVE:
                self.logger.debug("Removing unwanted lines")
                markdown_content = self._remove_markdown_lines(markdown_content, self.config.LINES_TO_REMOVE)

            # Save the markdown with the correct filename
            self.logger.debug(f"Saving page id '{page_id}' as filename: '{final_md_output_name}'")

            # Ensure the directory exists
            base_dir = os.path.dirname(md_output_name)
            final_out_path = os.path.join(base_dir, final_md_output_name)
            os.makedirs(os.path.dirname(final_out_path), exist_ok=True)

            with open(final_out_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)

            if not os.path.exists(final_out_path):
                raise FileNotFoundError(f"Output file not created: {final_out_path}")

            output_size = os.path.getsize(final_out_path)
            self.logger.info(f"Conversion successful. Output file size: {output_size} bytes")
            return True

        except Exception as e:
            self.logger.error(f"Conversion failed for {html_file}", exc_info=True)
            self.logger.debug(f"Error details: {str(e)}")
            return False

    def create_tag_mapping_from_html(self, input_folders: list, link_checker: LinkChecker) -> None:
        """Create tag mapping by scanning all HTML files for content-by-label sections."""
        self.logger.info("Creating tag mapping from HTML content-by-label sections...")
                
        for input_folder in input_folders:
            for root, _, files in os.walk(input_folder):
                # Skip special folders
                if self._is_special_folder(root):
                    continue
                    
                html_files = [f for f in files if f.endswith('.html')]
                
                for filename in html_files:
                    html_file = os.path.join(root, filename)
                    
                    try:
                        with open(html_file, 'r', encoding='utf-8') as f:
                            html_content = f.read()
                        
                        # Extract tags and map them to target pages
                        self._extract_tags_from_content_by_label_sections(html_content, link_checker)
                        
                    except Exception as e:
                        self.logger.error(f"Error processing {html_file} for tag mapping: {e}")
        
        self.logger.info(f"Tag mapping created with {len(self.page_tag_mapping)} target pages")

    def count_html_files(self, input_folders: list) -> int:
        """Count HTML files excluding special folders"""
        total_count = 0
        for input_folder in input_folders:
            for root, _, files in os.walk(input_folder):
                # Skip special folders when counting
                if self._is_special_folder(root):
                    continue

                # Count HTML files in this directory
                html_files = [f for f in files if f.endswith('.html')]
                total_count += len(html_files)

        return total_count
